---
title: "Modeling count data"
teaching: 20
exercises: 0
---

:::::::::::::::::::::::::::::::::::::: questions 

- What is a generalized linear model (GLM) and how can count data be represented with it?
- How can tests for association be implemented with GLMs? 
- How can GLMs help to account for biological variance? 

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Provide a workflow for modeling count data with replicates.

::::::::::::::::::::::::::::::::::::::::::::::::

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

**NOTE: This episode is work in progress.**

# How to model fractions
Let's consider the example from the previous lesson, and focus on the fractions of round cells in the control group only.

```{r echo=FALSE}
rep1 <- data.frame(
  replicate = rep("1",4),
  condition = c(rep("ctrl",2), rep("trt",2)),
  shape = c("round","stretched", "round", "stretched"),
  count = c(9, 39, 21 ,42)
)

rep2 <- data.frame(
  replicate = rep("2",4),
  condition = c(rep("ctrl",2), rep("trt",2)),
  shape = c("round","stretched", "round", "stretched"),
  count = c(16, 59, 51, 99)
)
  
rep3 <- data.frame(
  replicate = rep("3",4),
  condition = c(rep("ctrl",2), rep("trt",2)),
  shape = c("round","stretched", "round", "stretched"),
  count = c(7, 40, 22, 41)
)

tidy_table <- rbind(rep1, rep2, rep3)
```

Remember that our `data.frame` with the counts looks like this:
```{r}
tidy_table
```


```{r code-up-table, echo=FALSE}
table1 <- rbind(c(9, 39), c(21,42))
table2 <-  rbind(c(16, 59), c(51,99))
table3 <- rbind(c(7, 40), c(22,41))

table3d <- array(c(table1, table2, table3), dim = c(2,2,3))
table3d
```




We combine the counts from each replicate to calculate totals and observed fractions:
```{r}
tidy_data <- tidy_table %>% 
  pivot_wider(
    names_from = shape,
    values_from = count
  ) %>% 
  mutate(total = round + stretched) %>% 
  mutate(fraction = round/total)

tidy_data
```

Then can extract the `ctrl` counts only:
```{r}
ctrl_data <- tidy_data %>% 
  filter(condition == "ctrl") 
ctrl_data
```

We can model the fraction with a GLM of the binomial family:

```{r}
glm.binom <- glm(cbind(round,stretched) ~ 1 , 
      data= ctrl_data,
      family= binomial("logit"))
coefficients(glm.binom)
```

**Model formula:** This GLM models fractions as a function of the variables we supply behind the `~` sign in the model formula. In our case, we didn't give any variables, just an intercept (denoted by `1`). The intercept therefore represents the fraction of round cells estimated from the data. 

**Logit link:** Linear models assume that data are Gaussian distributed around their predictions. For fractional data, this is not the case. For this reason, the prediction happens on a *logit*-transformed level. We model 

$$\mu = \text{logit}(X\beta)$$,

where $\mu$ is the predicted count, and $X\beta$ is the *linear predictor*, a linear combination of the variables $X$ and the coefficients $\beta$ (including the intercept). 

If all this doesn't sound familiar to you, the important bit is that the coefficients that the GLM of a binomial family returns to us need to be *transformed* in order to be interpretable. 

The logit of a probability $p$ is given by $\ln(\frac{p}{1-p})$. It's also called the *log odds*.

The transformation from a value $x$ on the logit scale to fractions is:

$$\text{fraction} = \frac{\exp(x)}{1 + \exp(x)}$$
Let's transform our intercept into a fraction:

```{r}
x <- coefficients(glm.binom)
exp(x)/(1+exp(x))
```

We can compare this fraction with what we get by calculating a fraction from pooled counts:
```{r}
sum(ctrl_data$round) / sum(ctrl_data$total)
```


We learn: The GLM of the binomial offers an alternative way to calculate fractions. 

# How to model odds ratios

Models are good for determining how observations depend on variables. Observations in our case are fractions, and a meaningful variable can be the treatment. We can add it in the model formula as shown below. We use the full data set, not the one filtered for control data only.

```{r}
glm.binom.1var <- glm(cbind(round,stretched) ~ condition , 
      data= tidy_data,
      family= binomial("logit"))
```

This model gives us two coefficients: 
```{r}
coefficients(glm.binom.1var)
```

`Intercept` is the logit-transformed fraction (log odds) of round cells in the reference state, which is the `ctrl` group. By default, R sets the reference state by alphabetical order, and `ctrl` is before `trt`.  
`conditiontrt` is the coefficient which describes how the log odds for being round *change* when the condition is `trt` instead of `ctrl`. 

We can combine the two coefficients in the linear predictor $X\beta$ to calculate the fraction for treated cells.

```{r}
xb <- sum(coefficients(glm.binom.1var))
exp(xb)/(1+exp(xb))
```

Let's compare to the pooled fraction of round cells in the treatment condition:

```{r}
trt_data <- tidy_data %>% filter(condition == "trt")
sum(trt_data$round) / sum(trt_data$total)
```

The `conditiontrt` coefficient can also be interpreted as a log odds ratio. We can calculate the observed log odds ratio on pooled data, which is given by

$\log(\frac{n_{11} n_{22}}{n_{12}n_{21}})$.

Here is the estimate from the data:
```{r}
observed_odds_ratio <- sum(ctrl_data$round) * sum(trt_data$stretched) / (sum(ctrl_data$stretched)* sum(trt_data$round))

observed_log_odds <- log(observed_odds_ratio)

observed_log_odds
```

It coincides with the `conditiontrt` coefficient with a flipped sign:
```{r}
coefficients(glm.binom.1var)[2]
```

If you exchange the first and second column of the table (or the first and second row), the log odds ratio will also flip sign.

### Mathematical explanation

We know that 

- the coefficient `Intercept` gives the log odds for being round in the control condition: $\text{Int}=\text{log odds}_{ctrl}$ 
- the sum of the coefficients `conditiontrt` and `Intercept` give the log odds ratio for being round in the treatment condition: $\text{Intercept}+ \text{conditiontrt} = \text{log odds}_{trt}$

Therefore the coefficient `conditiontrt` can be expressed as 

$$\text{conditiontrt} = \text{log odds}_{ctrl} - \text{log odds}_{trt} = log(\frac{odds_{ctrl}}{odds_{trt}})$$
We learn that GLMs of the binomial family allow us to estimate odds ratios.

### Connection to Chi-square / Fisher test

Set up pooled table:
```{r}
pooled_table <- rbind(
  c(sum(ctrl_data$round) , sum(ctrl_data$stretched)),
    c(sum(trt_data$round) ,sum(trt_data$stretched))
)
pooled_table
```

```{r}
chisq.test(pooled_table, correct = FALSE)
```

```{r}
null_model <- glm(cbind(round,stretched) ~ 1 , 
      data= tidy_data,
      family= binomial("logit"))
```

```{r}
anova(null_model, glm.binom.1var, test = "Rao")
```


**Conclusion**: A chi-square test is a special case of a GLM. 
Testing for the parameter `conditiontrt`, which tells us how different the fractions for control and treatment are, is the same as testing for association of the variables condition and morphology.

(See also [here](https://lindeloev.github.io/tests-as-linear/#7_proportions:_chi-square_is_a_log-linear_model) how to reproduce the result of a chi-square test with a GLM of the poisson family.)


# How to add replicates 

Let's add the replicates to the model:

```{r}
glm.binom.repl <- glm(cbind(round,stretched) ~ condition + replicate , 
      data= tidy_data,
      family= binomial("logit")
)

summary(glm.binom.repl)
```
This model estimates the fractions for each replicate separately.
It say that the effect of the treatment is identical for all replicates (i.e. assumes homogeneous association), and calculates a separate effect of the replicate.

We can compare to the model that only considers the impact of the replicate on the fraction of round cells, but not of the condition. 

```{r}
glm.repOnly <- glm(cbind(round,stretched) ~ replicate , 
      data= tidy_data,
      family= binomial("logit")
)
```

```{r}
anova(glm.repOnly, glm.binom.repl, test = "Rao")
```

And we can compare to the Cochran-Mantel-Haenszel test.
```{r}
mantelhaen.test(table3d)
```

We've seen three methods to answer the same question: Does the condition have an impact on the fraction of round cells, when controlling for the effect of the replicate (and assuming the replicate doesn't have alter the effect of the condition)?  

- Use a Wald test for the `conditiontrt` coefficient, given by `summary(glm.binom.repl)`. It yields a p-value of $~0.0007$. 
- Compare the models with the formulae `~replicate` (`glm.repOnly`) and `~condition + replicate` (`glm.repl`) using the `anova` function. This yields a p-value of $~0.0006$. 
- Use the Cochran-Mantel-Haenszel test, which gives a p.value of $~0.0009$. 

We see that the methods are not identical, but give extremely similar results and lead to the same conclusion: When controlling for the replicate (stratifying the analysis), we seen a clear effect of the condition on the fraction of round cells.



## Three-way interaction

If the model `glm.binom.repl` doesn't fit well, this is evidence that the replicate has an impact on the effect of the treatment.
Finally, we really need the GLM, because there is no off-the-shelf test that we can apply to test for three-way interaction.

A model with three-way interaction looks like this:
```{r}
glm.threeway <- glm(cbind(round,stretched) ~ condition * replicate , 
      data= tidy_data,
      family= binomial("logit")
)
summary(glm.threeway)
```

We see no evidence that the replicate has an impact on the odds ratio, because the coefficients `conditionttrt:replicate2` and `conditiontrt:replicate3` are not significant.


# How to check for overdispersion

We have three replicates, so for each condition, we have three observed fractions of round cells. 

Let's visualize this: 

```{r}
tidy_data %>% 
  ggplot(aes(x=condition, y=fraction))+
  geom_point()
```
It's normal that for lower counts, the fractions are jumping around more.
For eyeballing purposes, it's therefore recommended to use stacked bar plots.



The question is whether the counts vary more than expected by a binomial model.

### Intuitive approach

- Compare expected to observed variance and calculate a ratio of these 
- don't over-interpret, because we calculate this from 3 replicates only 
- show exemplary for control

```{r, include=FALSE}
# filter data
ctrl_data = tidy_data %>% filter(condition == "ctrl") 

# calculate expected variance in success probability for each replicate
var_p_expect_ctrl = ctrl_data$fraction*(1-tidy_data$fraction)/tidy_data$total

# mean
mean_var_p_expect_ctrl = mean(var_p_expect_ctrl)

# calculate variance in observed success probabilities
var_p_obs_ctrl = var(tidy_data$fraction)
#var_p_obs_ctrl

#calculate oversipersion ratio for control replicates
# should be around 1 for no overdispersion
overdisrat_ctrl  = var_p_obs_ctrl/mean_var_p_expect_ctrl
overdisrat_ctrl
```

*To be added*: Source, or theory for this?

### Determine overdispersion through model

Take the model without the replicate and check for overdispersion:

```{r}
library(performance)
check_overdispersion(glm.binom.1var)
```
Or the one with the replicate included:
```{r}
check_overdispersion(glm.binom.repl)
```
For our cell data, we are fine. 
